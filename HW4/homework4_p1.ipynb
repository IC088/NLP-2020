{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ok9xpaI-uyUT"
   },
   "source": [
    "![sutd](sutd.png)\n",
    "## <center>50.040 Natural Language Processing, Summer 2020<center>\n",
    "<center>**Homework 4**\n",
    "\n",
    "<center>**Due 31 July 2020, 5pm** <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WHLjIQokuyUV"
   },
   "source": [
    "**Write your student ID and name**\n",
    "\n",
    "ID: 1003056\n",
    "\n",
    "Name: Ivan Christian\n",
    "\n",
    "Students whom you have discussed with (if any): Ng Jen Yang, Tee Zhi Yao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7rJYYMnuyUW"
   },
   "source": [
    "### Requirements:\n",
    "- Use Python to complete this homework.\n",
    "- Please list students with whom you have discussed (if any).\n",
    "- Follow the honor code strictly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "joGu6trB8RRN"
   },
   "source": [
    "In this homework, we'll implement ``IBM Model 1`` using the ``expectation–maximization (EM)`` algorithm. We need to estimate the  translation probabilities  $t(f|e)$ on a parallel corpus, where $e$ is a word from the English sentences and $f$ is a word from the corresponding foreign sentences. \n",
    "\n",
    "Note that there's a constraint for such probabilities:\n",
    "$$\\sum_f t(f|e)=1 , \\ \\ \\  t(f|e) \\ge 0  \\quad (1)$$\n",
    "\n",
    "**We'll use this constraint when initializing the translation probabilities in subsequent tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stJOvS_A8RRO"
   },
   "source": [
    "## Data\n",
    "We'll use the English-French parallel corpus under the folder ``data/part1``, which contains a set of translation instances. As can be seen below each instance consists of an English-French sentence pair (note that we are translating from French into English, but as we discussed in class, when working on the translation model using IBM model 1, we are interested in generating French from English).\n",
    "\n",
    "\n",
    "\n",
    "    Hop in.\tMontez.\n",
    "    Hug me.\tSerre-moi dans tes bras !\n",
    "    I left.\tJe suis parti.\n",
    "\n",
    "The dataset is obtained from [MXNET](http://data.mxnet.io/data/fra-eng.zip). Please run the provided code below to obtain the preprocessed English sentences and French sentences. Do not perform any further preprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from time import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Statistical Machine Translation \\[25 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "soTtZXt68RRT"
   },
   "outputs": [],
   "source": [
    "path = 'data/part1/en-fr.txt'\n",
    "with open(path, 'r', encoding='utf8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "#Original code from \n",
    "#https://www.d2l.ai/chapter_recurrent-neural-networks\n",
    "def preprocess_nmt(text):\n",
    "    '''\n",
    "    Arg:\n",
    "        text: parallel text, string\n",
    "    Return:\n",
    "        out: preprocessed text, string\n",
    "    '''\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "    no_space = lambda char, prev_char: (\n",
    "        True if char in (',', '!', '.') and prev_char != ' ' else False)\n",
    "    out = [' '+char if i > 0 and no_space(char, text[i-1]) else char\n",
    "           for i, char in enumerate(text.lower())]\n",
    "    out = ''.join(out)\n",
    "    return out\n",
    "\n",
    "def tokenize_nmt(text, num_examples = None):\n",
    "    '''\n",
    "    Args:\n",
    "        text: parallel text, string\n",
    "        num_examples: number of examples to be selected, int\n",
    "    Returns:\n",
    "        left: English sentences, list\n",
    "        right: French sentences, list\n",
    "    '''\n",
    "    left, right = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples: break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            left.append(parts[0].split(' '))\n",
    "            right.append(parts[1].split(' '))\n",
    "    return left, right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOUt-g3B8RRV"
   },
   "outputs": [],
   "source": [
    "#English sentences and corresponding French sentences\n",
    "#Each sentence has been preprocessed and tokenized\n",
    "text = preprocess_nmt(raw_text)\n",
    "english_sents, french_sents = tokenize_nmt(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['go', '.'],\n",
       "  ['hi', '.'],\n",
       "  ['run', '!'],\n",
       "  ['run', '!'],\n",
       "  ['who?'],\n",
       "  ['wow', '!'],\n",
       "  ['fire', '!'],\n",
       "  ['help', '!'],\n",
       "  ['jump', '.'],\n",
       "  ['stop', '!']],\n",
       " [['va', '!'],\n",
       "  ['salut', '!'],\n",
       "  ['cours', '!'],\n",
       "  ['courez', '!'],\n",
       "  ['qui', '?'],\n",
       "  ['ça', 'alors', '!'],\n",
       "  ['au', 'feu', '!'],\n",
       "  ['à', \"l'aide\", '!'],\n",
       "  ['saute', '.'],\n",
       "  ['ça', 'suffit', '!']])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sents[:10], french_sents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCTLIntg8RRX"
   },
   "source": [
    "\n",
    "### Quesiton 1 (3 points)\n",
    "1. Implement ``word_pairs_in_corpus`` which finds out all the possible word pairs (alignments) $(e, f)$ that appear in all the instances of the English-French dataset ``english_sents``, ``french_sents``. Note that we need to pad each English sentence with the special token \"NULL\" at the beginning.\n",
    "2. List down the 10 most frequent pairs. \n",
    "3. Count the number of unique pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_pairs_in_corpus(en_sents, fr_sents):\n",
    "    '''\n",
    "    params:\n",
    "        en_sents: list[list[str]]\n",
    "        fr_sents: list[list[str]]\n",
    "    return:\n",
    "        align_counts: Dict()--- key: (english_word, french_word), value: counts of the word pair in the corpus\n",
    "    '''\n",
    "    align_counts = None\n",
    "    # YOUR CODE HERE\n",
    "    align_counts = defaultdict()\n",
    "#     print(len(en_sents), len(fr_sents))\n",
    "    \n",
    "    sents_len = len(en_sents)\n",
    "    for idx in range(sents_len):            \n",
    "        \n",
    "        for en_word in en_sents[idx]:\n",
    "            for fr_word in fr_sents[idx]:\n",
    "                if (en_word, fr_word) in align_counts:\n",
    "                    align_counts[(en_word, fr_word)] += 1\n",
    "                else:\n",
    "                    align_counts[(en_word, fr_word)] = 1\n",
    "    # END OF YOUR CODE\n",
    "    return Counter(align_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(('.', '.'), 136734),\n",
       "  (('NULL', '.'), 135221),\n",
       "  (('i', '.'), 43189),\n",
       "  (('NULL', 'je'), 39821),\n",
       "  (('.', 'je'), 39096),\n",
       "  (('NULL', 'de'), 35073),\n",
       "  (('i', 'je'), 34415),\n",
       "  (('to', '.'), 31647),\n",
       "  (('.', 'de'), 30490),\n",
       "  (('the', '.'), 29170)],\n",
       " 1402126)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sents = [['NULL'] + sent for sent in english_sents]\n",
    "align_counts = word_pairs_in_corpus(english_sents, french_sents)\n",
    "align_counts.most_common(10), len(align_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = set([item[0] for item in align_counts.keys()])\n",
    "fr_vocab = set([item[1] for item in align_counts.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17430, 29741)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_vocab), len(fr_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (2 points):\n",
    "\n",
    "Implment the ``corpus_log_prob `` that computes the log probability of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_log_prob(en_sents, fr_sents, t):\n",
    "    '''\n",
    "    params:\n",
    "        en_sents: list[list[str]]\n",
    "        fr_sents: list[list[str]]\n",
    "        t: Dict() --- contains translation probabilities. For example, t[(english_word, french_word)] = p\n",
    "    return:\n",
    "        logp: float --- log probability of the corpus\n",
    "    '''\n",
    "    logp = 0\n",
    "    ### YOUR CODE HERE\n",
    "    for en_sent, fr_sent in zip(en_sents, fr_sents):\n",
    "        for fr_word in fr_sent:\n",
    "            for en_word in en_sent:\n",
    "            \n",
    "                p = t[(en_word, fr_word)]\n",
    "                \n",
    "                logp += np.log(p)\n",
    "    \n",
    "    # END OF YOUR CODE\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "424sQt3b8RRZ"
   },
   "source": [
    "## Hard EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WB_5EV3q8RRa"
   },
   "source": [
    "### Question 3 (10 points)\n",
    "Based on the word pairs obtained in Q1, implement ``Hard EM algorithm`` to calculate the  translation probabilities  $t(f|e)$ on the English-French corpus. \n",
    "\n",
    "It is possible that in the hard EM algorithm a word $\\tilde{e}$ from an English sentence may not be aligned with any word from the corresponding French sentence. In this case, let us set the corresponding probabilities $t(f|\\tilde{e})=\\frac{1}{|V_f|}$ where $|V_f|$ is the size of the French vocabulary (in this case, the number of unique French words that ever appear in the training parallel corpus).\n",
    "\n",
    "1. Implement ``init`` function which initializes the translation probability dictionary $t$ according to equation (1). You need to use ``numpy.random.rand()`` in this part.\n",
    "2. Implement ``hard_EM`` function which runs one ``Expectation/Maximization`` iteration.\n",
    "3. Run the training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(word_pairs):\n",
    "    '''\n",
    "    Use np.random.rand() to initialize translation probabilities t(f|e)\n",
    "    params:\n",
    "        word_pairs: List[(str, str)] --- list of word pairs\n",
    "    return:\n",
    "        t: Dict(), key: (english_word, french_word), value: the initial probability t(f|e). For example, t[(a, un)] = 0.5\n",
    "    '''\n",
    "    np.random.seed(5)\n",
    "    t = dict()\n",
    "    ### YOUR CODE HERE\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    t = defaultdict(float)\n",
    "    value = defaultdict(float)\n",
    "    \n",
    "    \n",
    "    for en_word, fr_word in word_pairs:\n",
    "        rnd = np.random.rand()\n",
    "        value[en_word] += rnd\n",
    "        t[(en_word, fr_word)] = rnd\n",
    "        \n",
    "    for pair in word_pairs:\n",
    "        t[pair] /= value[pair[0]]\n",
    "        \n",
    "    ### END OF YOUR CODE\n",
    "    return dict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_EM(en_sents, fr_sents, fr_vocab, t):\n",
    "    '''\n",
    "    One 'Expectation', 'Maximization' iteration.\n",
    "    params:\n",
    "        en_sents: List[List[str]]\n",
    "        fr_sents: List[List[str]]\n",
    "        fr_vocab: int --- size of the French vocab\n",
    "        t: Dict() --- translation probability dictionary from last iteration\n",
    "        \n",
    "    return:\n",
    "        new_t: Dict() --- updated parameters, dictionary\n",
    "    '''\n",
    "    new_t = t\n",
    "    ### YOUR CODE HERE\n",
    "    import operator\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    most_probable = {}\n",
    "    value = defaultdict(float)\n",
    "    out = dict()\n",
    "    align_count_updated = dict()\n",
    "    for key, value in t.items():\n",
    "        en_word, fr_word = key\n",
    "        out.setdefault(en_word, {})[fr_word] = value\n",
    "        \n",
    "    align_counts = word_pairs_in_corpus(en_sents, fr_sents)\n",
    "    for key, value in align_counts.items():\n",
    "        en_word, fr_word = key\n",
    "        align_count_updated.setdefault(en_word, {})[fr_word] = value\n",
    "    # E step\n",
    "    for en_v in en_vocab:\n",
    "        # find max from the new dict\n",
    "        argmax_trans = max(out[en_v].items(), key=operator.itemgetter(1))\n",
    "\n",
    "        most_probable.setdefault(en_v, {})[argmax_trans[0]] = argmax_trans[1]  \n",
    "    #M Step \n",
    "    count_en = {k: sum(v.values()) for k, v in align_count_updated.items()}\n",
    "    \n",
    "    for en_sent, fr_sent in tqdm(zip(en_sents,fr_sents)):\n",
    "        for en_word in en_sent:\n",
    "            for fr_word in fr_sent:\n",
    "                if en_word in most_probable.keys() :\n",
    "                    if fr_word in most_probable[en_word].keys():\n",
    "                        new_t[(en_word,fr_word)] = align_counts[(en_word,fr_word)]/count_en[en_word]\n",
    "#                     else:\n",
    "#                         new_t[(en_word,fr_word)] = -10**8\n",
    "                else:\n",
    "                    new_t[(en_word,fr_word)] = 1/len(fr_vocab) \n",
    "                \n",
    "#     for pair in word_pair_update:\n",
    "#         new_t[pair] /= count_en[pair[0]]\n",
    "    \n",
    "    \n",
    "    ### END OF YOUR CODE\n",
    "    \n",
    "    return new_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167130it [00:02, 76127.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective Function: -95137007.12011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167130it [00:02, 75734.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective Function: -95144898.00715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167130it [00:02, 73852.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective Function: -95143427.22856\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# Randomly initialized the probabilities under the contraint\n",
    "############################################################\n",
    "hard_t = init(list(align_counts.keys()))\n",
    "    \n",
    "##################\n",
    "# Hard EM training\n",
    "##################\n",
    "iteration = 0\n",
    "while iteration < 10:\n",
    "    \n",
    "    logp = corpus_log_prob(english_sents, french_sents, hard_t)    \n",
    "    hard_t = hard_EM(english_sents, french_sents, fr_vocab, hard_t)\n",
    "    print('Objective Function:', round(logp, 5))\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18sBQ---8RRc"
   },
   "source": [
    "### Visualization\n",
    "Using 2D-heatmap, visualize the translation probability (namely $t(f|e)$) for each of the instances below:\n",
    "\n",
    "    NULL tom loves chocolate .   tom adore le chocolat .    \n",
    "    NULL it was a very exciting game .   c'était un jeu vraiment très excitant .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trans_prob(en, fr, t):\n",
    "    '''\n",
    "    Visualize the translation probability of an instance\n",
    "    '''\n",
    "    alignments = np.zeros([len(fr), len(en)])\n",
    "    for i in range(len(fr)):\n",
    "        for j in range(len(en)):\n",
    "            alignments[i, j] = t[(en[j], fr[i])]\n",
    "    sns.heatmap(alignments, cmap='PuBuGn', annot=True)\n",
    "    _, _ = plt.yticks(np.arange(len(fr))+0.5, fr, rotation=0, fontsize=10)\n",
    "    _, _ = plt.xticks(np.arange(len(en))+0.5, en, rotation=30, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = \"NULL tom loves chocolate .\".split()\n",
    "fr = \"tom adore le chocolat .\".split()\n",
    "visualize_trans_prob(en, fr, hard_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "en = \"NULL it was a very exciting game .\".split()\n",
    "fr = \"c'était un jeu vraiment très excitant .\".split()\n",
    "visualize_trans_prob(en, fr, hard_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azApJ9p78RRd"
   },
   "source": [
    "## Soft EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUYEBjlO8RRe"
   },
   "source": [
    "### Question 4 (10 points)\n",
    "\n",
    "1. Implement ``soft_EM`` function which runs one ``Expectation/Maximization`` iteration.\n",
    "2. Run the training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_EM(en_sents, fr_sents, t):\n",
    "    '''\n",
    "    params:\n",
    "        en_sents: English sentences, list\n",
    "        fr_sents: foreign sentences, list\n",
    "    return:\n",
    "        t: updated parameters, dictionary\n",
    "    '''\n",
    "    new_t = t\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### END OF YOUR CODE\n",
    "    return new_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the algorithm first using the objective value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Randomly initialized the probabilities under the contraint\n",
    "############################################################\n",
    "soft_t = init(list(align_counts.keys()))\n",
    "    \n",
    "##################\n",
    "# Hard EM training\n",
    "##################\n",
    "iteration = 0\n",
    "while iteration < 15:\n",
    "    \n",
    "    logp = corpus_log_prob(english_sents, french_sents, soft_t)    \n",
    "    soft_t = soft_EM(english_sents, french_sents, soft_t)  \n",
    "    print('Objective Function:', round(logp, 5))\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = \"NULL tom loves chocolate .\".split()\n",
    "fr = \"tom adore le chocolat .\".split()\n",
    "visualize_trans_prob(en, fr, soft_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "en = \"NULL it was a very exciting game .\".split()\n",
    "fr = \"c'était un jeu vraiment très excitant .\".split()\n",
    "visualize_trans_prob(en, fr, soft_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "homework 3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
